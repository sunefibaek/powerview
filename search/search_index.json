{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Powerview","text":"<p>Powerview is a comprehensive data stack for collecting, storing, and analyzing detailed electricity consumption data from the Danish Eloverblik platform.</p> <p>Created for users who want full ownership of their energy data, Powerview extracts historical and ongoing consumption metrics, standardizes them into open formats, and provides a powerful analytics layer for visualization.</p>"},{"location":"#how-it-works","title":"How it works","text":"<ol> <li>Ingest: A Python pipeline fetches hourly consumption data from the Eloverblik API.</li> <li>Store: Data is saved as partitioned Parquet files, creating an efficient, immutable data lake on your local disk.</li> <li>Model: A DuckDB analytics database creates curated views on top of the raw files, handling data cleaning, aggregations (daily/monthly), and enrichment.</li> <li>Visualize: An included Apache Superset configuration connects to the DuckDB layer, offering professional-grade dashboards and exploration tools.</li> </ol>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Data Ownership: All data is stored locally in open parquet formats. Your usage history is yours to keep, independent of platform changes.</li> <li>Incremental Loading: Intelligent state tracking ensures only new data is fetched, respecting API limits and optimizing performance.</li> <li>Modern Stack: Built on the efficient combination of DuckDB and Parquet (OLAP), capable of handling years of hourly data with minimal resource usage.</li> <li>Ready-to-use Dashboards: Comes with Superset setup to visualize trends, heatmaps, and load profiles out of the box.</li> </ul>"},{"location":"#use-cases-automation","title":"Use Cases &amp; Automation","text":""},{"location":"#automated-daily-ingestion","title":"Automated Daily Ingestion","text":"<p>Powerview is designed to run unattended. You can schedule the ingestion script to run daily (e.g., at 08:00 UTC when data is usually available) using standard system tools.</p> <p>Example Crontab:</p> <pre><code>0 8 * * * cd /home/user/powerview &amp;&amp; poetry run python -m powerview.src.main &gt;&gt; /var/log/powerview.log 2&gt;&amp;1\n</code></pre>"},{"location":"#home-server-deployment","title":"Home Server Deployment","text":"<p>The stack is lightweight enough to run on a Raspberry Pi, Synology NAS, or any small Linux server. The Superset instance runs in Docker, and the data pipeline requires only a Python environment.</p>"},{"location":"#ad-hoc-data-science","title":"Ad-hoc Data Science","text":"<p>Because the data is stored in standard DuckDB and Parquet formats, you aren't limited to the built-in tools. You can connect directly to <code>duckdb/analytics.duckdb</code> using: - Jupyter Notebooks for advanced forecasting. - DBeaver or DataGrip for SQL exploration. - R or Excel for statistical analysis.</p> <p>Use the navigation on the left to get started, review data storage, or configure Superset.</p>"},{"location":"data-and-storage/","title":"Data and storage","text":""},{"location":"data-and-storage/#local-data-layout","title":"Local data layout","text":"<p>Data is stored under the top-level data/ directory with partitions for metering points and dates.</p> <p>Each partition contains one Parquet file:</p> <pre><code>data/\n    metering_point=&lt;METERING_POINT_ID&gt;/\n        date=&lt;YYYY-MM-DD&gt;/\n            consumption_data.parquet\n</code></pre>"},{"location":"data-and-storage/#parquet-schema","title":"Parquet schema","text":"<p>Each Parquet file contains hourly readings for a single metering point and a single day. The schema is flat and corresponds to the normalized API response:</p> <ul> <li><code>metering_point_id</code> (string): Metering point ID from Eloverblik.</li> <li><code>timestamp</code> (timestamp): Hourly reading timestamp (UTC).</li> <li><code>consumption_value</code> (float): Consumption value for the hour.</li> <li><code>quality</code> (string): Quality marker from the API (if present).</li> <li><code>unit</code> (string): Unit from the API (default <code>kWh</code>).</li> <li><code>ingestion_timestamp</code> (timestamp): When the record was ingested (UTC).</li> <li><code>ingestion_date</code> (date): Ingestion date (UTC).</li> </ul>"},{"location":"data-and-storage/#duckdb","title":"DuckDB","text":"<p>DuckDB files are stored at the repository root and under duckdb/.</p> <ul> <li><code>duckdb/analytics.duckdb</code>: Analytics/reporting database. Built from the Parquet data and     contains curated tables/views used for exploration and Superset dashboards. Use     create_analytics_db.py to (re)build it.</li> <li><code>state.duckdb</code>: Operational state database. Stores ingestion state (for example     last ingestion date per metering point) to support incremental extraction.</li> </ul>"},{"location":"data-and-storage/#analytics-database-views","title":"Analytics database views","text":"<p>The analytics database creates a reporting schema with default views:</p> <ul> <li><code>reporting.meter_data_stage</code>: Raw hourly readings with derived date/time fields     (date, hour, weekday, month, year) for downstream aggregations.</li> <li><code>reporting.meter_data_clean</code>: Filters out null or negative consumption values to     provide a clean base for metrics.</li> <li><code>reporting.daily_consumption</code>: Daily aggregates per metering point (total, average,     min, max, count).</li> <li><code>reporting.monthly_consumption</code>: Monthly totals per metering point with month-over-month     deltas and ratios for trend analysis.</li> <li><code>reporting.hourly_profile</code>: Average hourly usage by weekday to describe typical load     shapes.</li> <li><code>reporting.missing_data_summary</code>: Daily expected vs actual reading counts to highlight     missing hours.</li> <li><code>reporting.load_variability</code>: Daily variability metrics (hourly standard deviation and     peak usage) for volatility analysis.</li> <li><code>reporting.daily_quality_flags</code>: Daily flags for suspicious zero-consumption days     surrounded by non-zero readings.</li> <li><code>reporting.meter_metadata</code>: Metering point metadata loaded from metering_points.yml     for labeling and filtering.</li> <li><code>reporting.meter_metadata_enriched</code>: Daily consumption joined with metadata for     easy dashboarding.</li> </ul>"},{"location":"development/","title":"Development","text":""},{"location":"development/#code-style","title":"Code style","text":"<p>Ruff is used for linting and formatting.</p> <pre><code>ruff format .\nruff check . --fix\n</code></pre>"},{"location":"development/#tests","title":"Tests","text":"<pre><code>pytest\n</code></pre>"},{"location":"getting-started/","title":"Getting started","text":""},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.13+</li> <li>Poetry</li> <li>Docker (for Superset)</li> </ul>"},{"location":"getting-started/#install","title":"Install","text":"<pre><code>poetry install\n</code></pre>"},{"location":"getting-started/#run","title":"Run","text":"<pre><code>poetry run python -m powerview.src.main\n</code></pre>"},{"location":"getting-started/#verify","title":"Verify","text":"<pre><code>poetry run python verify.py\n</code></pre>"},{"location":"initial-setup/","title":"Initial setup","text":"<p>Initial setup requires an API key from Eloverblik. Sign in with MitID, open API Access (top-right menu), click Create token, give it a name, and copy the token. You will only be able to see the key once, after closing the modal.</p>"},{"location":"initial-setup/#1-configure-environment-variables","title":"1) Configure environment variables","text":"<p>Copy the example env file and add your token:</p> <pre><code>cp .env.example .env\n</code></pre> <p>Set the token in <code>.env</code>:</p> <pre><code>ELOVERBLIK_REFRESH_TOKEN=...your token...\n</code></pre> <p>Optional settings (defaults shown):</p> <ul> <li><code>DATA_STORAGE_PATH=./data</code></li> <li><code>ANALYTICS_DB_PATH=./duckdb/analytics.duckdb</code></li> <li><code>STATE_DB_PATH=./state.duckdb</code></li> <li><code>INITIAL_BACKFILL_DAYS=1095</code></li> </ul>"},{"location":"initial-setup/#2-define-metering-points","title":"2) Define metering points","text":"<p>Copy the sample file and register the metering point IDs you want to track:</p> <pre><code>cp metering_points.yml.example metering_points.yml\n</code></pre> <p>Fill in <code>name</code>, <code>type</code>, <code>location</code>, and <code>description</code>. These fields are used for labeling in the reporting layer.</p>"},{"location":"initial-setup/#3-install-dependencies","title":"3) Install dependencies","text":"<p>The project uses Poetry:</p> <pre><code>poetry install\n</code></pre>"},{"location":"initial-setup/#4-run-the-ingestion-pipeline","title":"4) Run the ingestion pipeline","text":"<p>Run the main module to fetch data and store Parquet files in <code>./data</code>:</p> <pre><code>poetry run python -m powerview.src.main\n</code></pre> <p>The first run performs a backfill based on <code>INITIAL_BACKFILL_DAYS</code>, then future runs incrementally ingest new data.</p>"},{"location":"reference/","title":"Reference","text":""},{"location":"reference/#api-client","title":"API client","text":"<p>API client module for Eloverblik API.</p>"},{"location":"reference/#powerview.src.api_client.get_meter_data","title":"<code>get_meter_data(access_token, date_from, date_to, metering_point_ids=None)</code>","text":"<p>Retrieve meter data for the specified date range with hourly aggregation.</p> <p>Parameters:</p> Name Type Description Default <code>access_token</code> <code>str</code> <p>The access token from get_access_token().</p> required <code>date_from</code> <code>str</code> <p>Start date in \"YYYY-MM-DD\" format.</p> required <code>date_to</code> <code>str</code> <p>End date in \"YYYY-MM-DD\" format.</p> required <code>metering_point_ids</code> <code>list[str] | None</code> <p>List of metering point IDs to fetch. If None or empty,                API will return data for all available metering points.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing meter data for requested metering points.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the request fails.</p> Source code in <code>powerview/src/api_client.py</code> <pre><code>def get_meter_data(\n    access_token: str, date_from: str, date_to: str, metering_point_ids: list[str] | None = None\n) -&gt; dict:\n    \"\"\"\n    Retrieve meter data for the specified date range with hourly aggregation.\n\n    Args:\n        access_token: The access token from get_access_token().\n        date_from: Start date in \"YYYY-MM-DD\" format.\n        date_to: End date in \"YYYY-MM-DD\" format.\n        metering_point_ids: List of metering point IDs to fetch. If None or empty,\n                           API will return data for all available metering points.\n\n    Returns:\n        Dictionary containing meter data for requested metering points.\n\n    Raises:\n        requests.HTTPError: If the request fails.\n    \"\"\"\n    url = f\"https://api.eloverblik.dk/CustomerApi/api/meterdata/gettimeseries/{date_from}/{date_to}/Hour\"\n    headers = {\n        \"Authorization\": f\"Bearer {access_token}\",\n        \"Content-Type\": \"application/json\",\n        \"api-version\": \"1.0\",\n    }\n    payload = {\n        \"meteringPoints\": {\"meteringPoint\": metering_point_ids or []},\n    }\n\n    logger.info(\"Requesting meter data from %s to %s with Hour aggregation\", date_from, date_to)\n\n    # Debug: Print curl equivalent command\n    headers_str = \" \".join([f'-H \"{k}: {v}\"' for k, v in headers.items()])\n    payload_str = json.dumps(payload)\n    curl_command = f\"curl -X 'POST' '{url}' {headers_str} -d '{payload_str}'\"\n    logger.debug(\"Equivalent curl command:\\n%s\", curl_command)\n\n    response = requests.post(url, headers=headers, json=payload)\n    response.raise_for_status()\n\n    return response.json()\n</code></pre>"},{"location":"reference/#powerview.src.api_client.get_meter_data_with_retry","title":"<code>get_meter_data_with_retry(access_token, date_from, date_to, metering_point_ids=None, max_retries=3)</code>","text":"<p>Fetch meter data with retry logic for rate limiting and service errors.</p> <p>Handles HTTP 429 (rate limited) and HTTP 503 (service unavailable) by waiting 1 minute and retrying. Other HTTP errors are raised immediately.</p> <p>Parameters:</p> Name Type Description Default <code>access_token</code> <code>str</code> <p>The access token.</p> required <code>date_from</code> <code>str</code> <p>Start date in \"YYYY-MM-DD\" format.</p> required <code>date_to</code> <code>str</code> <p>End date in \"YYYY-MM-DD\" format.</p> required <code>metering_point_ids</code> <code>list[str] | None</code> <p>List of metering point IDs to fetch. If None or empty,                API will return data for all available metering points.</p> <code>None</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retries (default: 3).</p> <code>3</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing meter data.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If retry exhausted or non-retryable error.</p> Source code in <code>powerview/src/api_client.py</code> <pre><code>def get_meter_data_with_retry(\n    access_token: str,\n    date_from: str,\n    date_to: str,\n    metering_point_ids: list[str] | None = None,\n    max_retries: int = 3,\n) -&gt; dict:\n    \"\"\"\n    Fetch meter data with retry logic for rate limiting and service errors.\n\n    Handles HTTP 429 (rate limited) and HTTP 503 (service unavailable) by\n    waiting 1 minute and retrying. Other HTTP errors are raised immediately.\n\n    Args:\n        access_token: The access token.\n        date_from: Start date in \"YYYY-MM-DD\" format.\n        date_to: End date in \"YYYY-MM-DD\" format.\n        metering_point_ids: List of metering point IDs to fetch. If None or empty,\n                           API will return data for all available metering points.\n        max_retries: Maximum number of retries (default: 3).\n\n    Returns:\n        Dictionary containing meter data.\n\n    Raises:\n        requests.HTTPError: If retry exhausted or non-retryable error.\n    \"\"\"\n    retries = 0\n    while retries &lt; max_retries:\n        try:\n            return get_meter_data(access_token, date_from, date_to, metering_point_ids)\n        except requests.exceptions.HTTPError as e:\n            if e.response.status_code in (429, 503):\n                retries += 1\n                if retries &gt;= max_retries:\n                    logger.error(\"Max retries exceeded for date range %s to %s\", date_from, date_to)\n                    raise\n                wait_time = 60\n                logger.warning(\n                    \"Rate limited or service unavailable. Waiting %ss before retry %s/%s\",\n                    wait_time,\n                    retries,\n                    max_retries,\n                )\n                time.sleep(wait_time)\n            else:\n                raise\n</code></pre>"},{"location":"reference/#powerview.src.api_client.get_metering_points","title":"<code>get_metering_points(access_token)</code>","text":"<p>Retrieve all available metering points for the authenticated user.</p> <p>Parameters:</p> Name Type Description Default <code>access_token</code> <code>str</code> <p>The access token from get_access_token().</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing metering points data from the API.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the request fails.</p> Source code in <code>powerview/src/api_client.py</code> <pre><code>def get_metering_points(access_token: str) -&gt; dict:\n    \"\"\"\n    Retrieve all available metering points for the authenticated user.\n\n    Args:\n        access_token: The access token from get_access_token().\n\n    Returns:\n        Dictionary containing metering points data from the API.\n\n    Raises:\n        requests.HTTPError: If the request fails.\n    \"\"\"\n    url = \"https://api.eloverblik.dk/CustomerApi/api/meteringpoints/meteringpoints\"\n    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n    logger.info(\"Requesting metering points from Eloverblik API\")\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n\n    return response.json()\n</code></pre>"},{"location":"reference/#authentication","title":"Authentication","text":"<p>Authentication module for Eloverblik API.</p>"},{"location":"reference/#powerview.src.auth.get_access_token","title":"<code>get_access_token(refresh_token)</code>","text":"<p>Exchange refresh token for an access token.</p> <p>The Eloverblik API uses a refresh token to obtain a temporary access token valid for 24 hours. A fresh token should be obtained once per extraction run.</p> <p>Parameters:</p> Name Type Description Default <code>refresh_token</code> <code>str</code> <p>The refresh token from environment variables.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The access token as a string.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the token exchange fails.</p> <code>KeyError</code> <p>If the API response doesn't contain the expected \"result\" field.</p> Source code in <code>powerview/src/auth.py</code> <pre><code>def get_access_token(refresh_token: str) -&gt; str:\n    \"\"\"\n    Exchange refresh token for an access token.\n\n    The Eloverblik API uses a refresh token to obtain a temporary access token\n    valid for 24 hours. A fresh token should be obtained once per extraction run.\n\n    Args:\n        refresh_token: The refresh token from environment variables.\n\n    Returns:\n        The access token as a string.\n\n    Raises:\n        requests.HTTPError: If the token exchange fails.\n        KeyError: If the API response doesn't contain the expected \"result\" field.\n    \"\"\"\n    url = \"https://api.eloverblik.dk/CustomerApi/api/token\"\n    headers = {\"Authorization\": f\"Bearer {refresh_token}\"}\n\n    logger.info(\"Requesting access token from Eloverblik API\")\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n\n    token = response.json()[\"result\"]\n    logger.info(\"Successfully obtained access token\")\n    return token\n</code></pre>"},{"location":"reference/#configuration","title":"Configuration","text":"<p>Configuration management module for Eloverblik data extraction.</p>"},{"location":"reference/#powerview.src.config.load_config","title":"<code>load_config()</code>","text":"<p>Load and validate configuration from .env and <code>metering_points.yml</code>.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Fully validated configuration suitable for pipeline execution.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the refresh token or metering-point definitions are missing.</p> Source code in <code>powerview/src/config.py</code> <pre><code>def load_config() -&gt; dict:\n    \"\"\"Load and validate configuration from .env and ``metering_points.yml``.\n\n    Returns:\n        dict: Fully validated configuration suitable for pipeline execution.\n\n    Raises:\n        ValueError: If the refresh token or metering-point definitions are missing.\n    \"\"\"\n\n    load_dotenv()\n    metering_points = load_metering_points()\n    metering_point_ids = {mp_data[\"name\"]: mp_id for mp_id, mp_data in metering_points.items()}\n\n    config = {\n        \"refresh_token\": os.getenv(\"ELOVERBLIK_REFRESH_TOKEN\"),\n        \"metering_points\": metering_points,\n        \"metering_point_ids\": dict(metering_point_ids),\n        \"data_storage_path\": os.getenv(\"DATA_STORAGE_PATH\", \"./data\"),\n        \"analytics_db_path\": os.getenv(\"ANALYTICS_DB_PATH\", \"./duckdb/analytics.duckdb\"),\n        \"state_db_path\": os.getenv(\"STATE_DB_PATH\", \"./state.duckdb\"),\n        \"log_level\": os.getenv(\"LOG_LEVEL\", \"INFO\"),\n        \"initial_backfill_days\": int(os.getenv(\"INITIAL_BACKFILL_DAYS\", \"1095\")),\n    }\n\n    if not config[\"refresh_token\"]:\n        logger.error(\"ELOVERBLIK_REFRESH_TOKEN is required\")\n        raise ValueError(\"ELOVERBLIK_REFRESH_TOKEN is required\")\n\n    config[\"valid_metering_points\"] = config[\"metering_point_ids\"]\n\n    if not config[\"valid_metering_points\"]:\n        logger.error(\"At least one metering point ID must be configured\")\n        raise ValueError(\"At least one metering point ID must be configured\")\n\n    logger.info(\n        \"Configuration loaded. Tracking %d metering point(s)\",\n        len(config[\"valid_metering_points\"]),\n    )\n\n    return config\n</code></pre>"},{"location":"reference/#powerview.src.config.load_metering_points","title":"<code>load_metering_points(file_path=None)</code>","text":"<p>Load metering point metadata from YAML configuration.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | None</code> <p>Optional explicit path to a YAML file.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, dict[str, Any]]</code> <p>dict[str, dict[str, Any]]: Mapping of metering point IDs to metadata.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file is missing, malformed, or empty.</p> Source code in <code>powerview/src/config.py</code> <pre><code>def load_metering_points(file_path: str | None = None) -&gt; dict[str, dict[str, Any]]:\n    \"\"\"Load metering point metadata from YAML configuration.\n\n    Args:\n        file_path: Optional explicit path to a YAML file.\n\n    Returns:\n        dict[str, dict[str, Any]]: Mapping of metering point IDs to metadata.\n\n    Raises:\n        ValueError: If the file is missing, malformed, or empty.\n    \"\"\"\n\n    resolved_path = _resolve_metering_points_path(file_path)\n    if not resolved_path.exists():\n        raise ValueError(f\"Metering points file not found: {resolved_path}\")\n\n    with resolved_path.open(\"r\", encoding=\"utf-8\") as handle:\n        try:\n            raw_data = yaml.safe_load(handle) or {}\n        except yaml.YAMLError as e:\n            raise ValueError(f\"Failed to parse YAML file {resolved_path}: {e}\") from e\n\n    if \"metering_points\" not in raw_data:\n        raise ValueError(\"Metering points file is missing the required 'metering_points' key\")\n    metering_points_data = raw_data[\"metering_points\"]\n    if not isinstance(metering_points_data, dict):\n        raise ValueError(\n            \"The 'metering_points' key must map to a dictionary (mapping) of metering points\"\n        )\n\n    normalized: dict[str, dict[str, Any]] = {}\n    for mp_id, metadata in metering_points_data.items():\n        mp_id_str = str(mp_id).strip()\n        if not mp_id_str:\n            continue\n\n        if metadata is None:\n            metadata = {}\n        elif not isinstance(metadata, dict):\n            raise ValueError(\n                f\"Metadata for metering point '{mp_id_str}' must be a mapping (dict), \"\n                f\"got {type(metadata).__name__}\"\n            )\n\n        name = metadata.get(\"name\") or mp_id_str\n        normalized[mp_id_str] = {**metadata, \"id\": mp_id_str, \"name\": name}\n\n    if not normalized:\n        raise ValueError(\"At least one metering point ID must be configured\")\n\n    logger.info(\n        \"Loaded %d metering point definition(s) from %s\",\n        len(normalized),\n        resolved_path,\n    )\n    return normalized\n</code></pre>"},{"location":"reference/#extraction","title":"Extraction","text":"<p>Data extraction and normalization module for Eloverblik data.</p>"},{"location":"reference/#powerview.src.extract.chunk_date_range","title":"<code>chunk_date_range(date_from, date_to, chunk_days=90)</code>","text":"<p>Split a date range into chunks to respect API limits.</p> <p>Automatically chunks date ranges into 90-day increments to ensure reliability and respect the API maximum of 730 days per request.</p> <p>Parameters:</p> Name Type Description Default <code>date_from</code> <code>date</code> <p>Start date.</p> required <code>date_to</code> <code>date</code> <p>End date.</p> required <code>chunk_days</code> <code>int</code> <p>Size of each chunk in days (default: 90).</p> <code>90</code> <p>Returns:</p> Type Description <code>list[tuple[date, date]]</code> <p>List of (chunk_from, chunk_to) date tuples.</p> Source code in <code>powerview/src/extract.py</code> <pre><code>def chunk_date_range(\n    date_from: date, date_to: date, chunk_days: int = 90\n) -&gt; list[tuple[date, date]]:\n    \"\"\"\n    Split a date range into chunks to respect API limits.\n\n    Automatically chunks date ranges into 90-day increments to ensure\n    reliability and respect the API maximum of 730 days per request.\n\n    Args:\n        date_from: Start date.\n        date_to: End date.\n        chunk_days: Size of each chunk in days (default: 90).\n\n    Returns:\n        List of (chunk_from, chunk_to) date tuples.\n    \"\"\"\n    chunks = []\n    current = date_from\n\n    while current &lt;= date_to:\n        chunk_end = min(current + timedelta(days=chunk_days - 1), date_to)\n        chunks.append((current, chunk_end))\n        if chunk_end &gt;= date_to:\n            break\n        current = chunk_end + timedelta(days=1)\n\n    logger.info(\"Split date range into %d chunk(s)\", len(chunks))\n    return chunks\n</code></pre>"},{"location":"reference/#powerview.src.extract.get_timeframe","title":"<code>get_timeframe(metering_point_id, initial_backfill_days=1095, db_path='./state.duckdb')</code>","text":"<p>Calculate the date range for data extraction based on DuckDB state.</p> <p>Logic: - If no prior ingestion exists, backfill from initial_backfill_days ago. - If prior ingestion exists, start from 7 days before (for overlap).</p> <p>Parameters:</p> Name Type Description Default <code>metering_point_id</code> <code>str</code> <p>The metering point ID.</p> required <code>initial_backfill_days</code> <code>int</code> <p>Initial backfill period in days (default: 1095).</p> <code>1095</code> <code>db_path</code> <code>str</code> <p>Path to the DuckDB database file.</p> <code>'./state.duckdb'</code> <p>Returns:</p> Type Description <code>tuple[date, date]</code> <p>Tuple of (date_from, date_to) as date objects.</p> Source code in <code>powerview/src/extract.py</code> <pre><code>def get_timeframe(\n    metering_point_id: str,\n    initial_backfill_days: int = 1095,\n    db_path: str = \"./state.duckdb\",\n) -&gt; tuple[date, date]:\n    \"\"\"\n    Calculate the date range for data extraction based on DuckDB state.\n\n    Logic:\n    - If no prior ingestion exists, backfill from initial_backfill_days ago.\n    - If prior ingestion exists, start from 7 days before (for overlap).\n\n    Args:\n        metering_point_id: The metering point ID.\n        initial_backfill_days: Initial backfill period in days (default: 1095).\n        db_path: Path to the DuckDB database file.\n\n    Returns:\n        Tuple of (date_from, date_to) as date objects.\n    \"\"\"\n    from powerview.src.storage import get_last_ingestion_date\n\n    last_date = get_last_ingestion_date(metering_point_id, db_path)\n\n    if last_date is None:\n        date_from = datetime.now(UTC).date() - timedelta(days=initial_backfill_days)\n        logger.info(\n            \"No prior ingestion for %s. Starting backfill from %s\",\n            metering_point_id,\n            date_from,\n        )\n    else:\n        date_from = last_date - timedelta(days=7)\n        logger.info(\n            \"Incremental ingestion for %s. Last ingestion was %s, starting from %s\",\n            metering_point_id,\n            last_date,\n            date_from,\n        )\n\n    date_to = datetime.now(UTC).date()\n    return date_from, date_to\n</code></pre>"},{"location":"reference/#powerview.src.extract.normalize_api_response","title":"<code>normalize_api_response(api_response, metering_point_ids)</code>","text":"<p>Normalize the complex nested Eloverblik API response into flat records.</p> <p>Extracts hourly consumption readings from the deeply nested API structure and flattens them to one record per reading. Filters to only requested metering points.</p> <p>Parameters:</p> Name Type Description Default <code>api_response</code> <code>dict</code> <p>The raw response from get_meter_data_with_retry().</p> required <code>metering_point_ids</code> <code>dict</code> <p>Dictionary of metering_point_id values to track.</p> required <p>Returns:</p> Type Description <code>list[dict]</code> <p>List of normalized record dicts with schema:</p> <code>list[dict]</code> <p>metering_point_id, timestamp, consumption_value, quality, unit,</p> <code>list[dict]</code> <p>ingestion_timestamp, ingestion_date</p> Source code in <code>powerview/src/extract.py</code> <pre><code>def normalize_api_response(api_response: dict, metering_point_ids: dict) -&gt; list[dict]:\n    \"\"\"\n    Normalize the complex nested Eloverblik API response into flat records.\n\n    Extracts hourly consumption readings from the deeply nested API structure\n    and flattens them to one record per reading. Filters to only requested\n    metering points.\n\n    Args:\n        api_response: The raw response from get_meter_data_with_retry().\n        metering_point_ids: Dictionary of metering_point_id values to track.\n\n    Returns:\n        List of normalized record dicts with schema:\n        metering_point_id, timestamp, consumption_value, quality, unit,\n        ingestion_timestamp, ingestion_date\n    \"\"\"\n    normalized_records = []\n    mp_id_set = set(metering_point_ids.values())\n\n    for result in api_response.get(\"result\", []):\n        if not result.get(\"success\"):\n            logger.warning(\"API result not successful: %s\", result.get(\"errorText\"))\n            continue\n\n        try:\n            market_doc = result.get(\"MyEnergyData_MarketDocument\", {})\n\n            for time_series in market_doc.get(\"TimeSeries\", []):\n                # Extract metering point ID\n                metering_point_id = (\n                    time_series.get(\"MarketEvaluationPoint\", {}).get(\"mRID\", {}).get(\"name\")\n                )\n\n                # Skip if not in our tracked set\n                if metering_point_id not in mp_id_set:\n                    continue\n\n                unit = time_series.get(\"measurement_Unit\", {}).get(\"name\", \"kWh\")\n\n                # Process each period\n                for period in time_series.get(\"Period\", []):\n                    period_start_str = period.get(\"timeInterval\", {}).get(\"start\")\n                    if not period_start_str:\n                        continue\n\n                    # Parse period start timestamp\n                    try:\n                        period_start = datetime.fromisoformat(\n                            period_start_str.replace(\"Z\", \"+00:00\")\n                        )\n                    except ValueError as e:\n                        logger.warning(\"Failed to parse period start timestamp: %s\", e)\n                        continue\n\n                    # Process each point (hourly readings)\n                    for point in period.get(\"Point\", []):\n                        try:\n                            position = int(point.get(\"position\", 0))\n                            value = float(point.get(\"out_Quantity.quantity\", 0))\n                            quality = point.get(\"out_Quantity.quality\", \"\")\n\n                            # Calculate timestamp: start + (position - 1) hours\n                            timestamp = period_start + timedelta(hours=position - 1)\n\n                            normalized_records.append(\n                                {\n                                    \"metering_point_id\": metering_point_id,\n                                    \"timestamp\": timestamp,\n                                    \"consumption_value\": value,\n                                    \"quality\": quality,\n                                    \"unit\": unit,\n                                    \"ingestion_timestamp\": datetime.now(UTC),\n                                    \"ingestion_date\": datetime.now(UTC).date(),\n                                }\n                            )\n                        except (ValueError, TypeError) as e:\n                            logger.warning(\"Failed to parse point data: %s\", e)\n                            continue\n        except Exception as e:\n            logger.error(\"Error processing API result: %s\", e)\n            continue\n\n    logger.info(\"Normalized %d records from API response\", len(normalized_records))\n    return normalized_records\n</code></pre>"},{"location":"reference/#storage","title":"Storage","text":"<p>Storage module for DuckDB state management and Parquet file I/O.</p>"},{"location":"reference/#powerview.src.storage.get_last_ingestion_date","title":"<code>get_last_ingestion_date(metering_point_id, db_path='./state.duckdb')</code>","text":"<p>Query the last ingestion date for a metering point from DuckDB.</p> <p>Returns None if no prior ingestion exists, enabling the extraction pipeline to determine whether to do full backfill or incremental.</p> <p>Parameters:</p> Name Type Description Default <code>metering_point_id</code> <code>str</code> <p>The metering point ID to query.</p> required <code>db_path</code> <code>str</code> <p>Path to the DuckDB database file.</p> <code>'./state.duckdb'</code> <p>Returns:</p> Type Description <code>date | None</code> <p>The last ingestion date as a date object, or None if no prior ingestion.</p> Source code in <code>powerview/src/storage.py</code> <pre><code>def get_last_ingestion_date(metering_point_id: str, db_path: str = \"./state.duckdb\") -&gt; date | None:\n    \"\"\"\n    Query the last ingestion date for a metering point from DuckDB.\n\n    Returns None if no prior ingestion exists, enabling the extraction\n    pipeline to determine whether to do full backfill or incremental.\n\n    Args:\n        metering_point_id: The metering point ID to query.\n        db_path: Path to the DuckDB database file.\n\n    Returns:\n        The last ingestion date as a date object, or None if no prior ingestion.\n    \"\"\"\n    try:\n        con = duckdb.connect(db_path)\n        result = con.execute(\n            \"SELECT last_ingestion_date FROM ingestion_state WHERE metering_point_id = ?\",\n            [metering_point_id],\n        ).fetchone()\n        con.close()\n\n        if result and result[0]:\n            return result[0]\n        return None\n    except Exception as e:\n        logger.warning(\"Failed to query last ingestion date for %s: %s\", metering_point_id, e)\n        return None\n</code></pre>"},{"location":"reference/#powerview.src.storage.init_duckdb_state","title":"<code>init_duckdb_state(db_path='./state.duckdb')</code>","text":"<p>Initialize the DuckDB state table if it does not exist.</p> <p>Creates an ingestion_state table to track the last ingestion date for each metering point, enabling incremental extraction.</p> <p>Parameters:</p> Name Type Description Default <code>db_path</code> <code>str</code> <p>Path to the DuckDB database file.</p> <code>'./state.duckdb'</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If database initialization fails.</p> Source code in <code>powerview/src/storage.py</code> <pre><code>def init_duckdb_state(db_path: str = \"./state.duckdb\") -&gt; None:\n    \"\"\"\n    Initialize the DuckDB state table if it does not exist.\n\n    Creates an ingestion_state table to track the last ingestion date\n    for each metering point, enabling incremental extraction.\n\n    Args:\n        db_path: Path to the DuckDB database file.\n\n    Raises:\n        Exception: If database initialization fails.\n    \"\"\"\n    try:\n        con = duckdb.connect(db_path)\n        con.execute(\n            \"\"\"\n            CREATE TABLE IF NOT EXISTS ingestion_state (\n                metering_point_id TEXT PRIMARY KEY,\n                last_ingestion_date DATE\n            )\n        \"\"\"\n        )\n        con.close()\n        logger.info(\"Initialized DuckDB state at %s\", db_path)\n    except Exception as e:\n        logger.error(\"Failed to initialize DuckDB state: %s\", e)\n        raise\n</code></pre>"},{"location":"reference/#powerview.src.storage.save_to_parquet","title":"<code>save_to_parquet(records, base_path=None)</code>","text":"<p>Save normalized records to partitioned Parquet files.</p> <p>Files are partitioned by metering_point_id and consumption date for efficient querying. If a file already exists, duplicate timestamps are replaced with new data (upsert strategy).</p> <p>Partition structure: /data/metering_point=/date=/consumption_data.parquet where date is the consumption date extracted from the timestamp. <p>Parameters:</p> Name Type Description Default <code>records</code> <code>list[dict]</code> <p>List of normalized record dicts.</p> required <code>base_path</code> <code>str | None</code> <p>Base directory for storing Parquet files. If None, uses        <code>data_storage_path</code> from config. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If write operation fails.</p> Source code in <code>powerview/src/storage.py</code> <pre><code>def save_to_parquet(records: list[dict], base_path: str | None = None) -&gt; None:\n    \"\"\"\n    Save normalized records to partitioned Parquet files.\n\n    Files are partitioned by metering_point_id and consumption date for efficient\n    querying. If a file already exists, duplicate timestamps are replaced with\n    new data (upsert strategy).\n\n    Partition structure: /data/metering_point=&lt;ID&gt;/date=&lt;YYYY-MM-DD&gt;/consumption_data.parquet\n    where date is the consumption date extracted from the timestamp.\n\n    Args:\n        records: List of normalized record dicts.\n        base_path: Base directory for storing Parquet files. If None, uses\n                   `data_storage_path` from config. Defaults to None.\n\n    Raises:\n        Exception: If write operation fails.\n    \"\"\"\n\n    if base_path is None:\n        from powerview.src.config import load_config\n\n        config = load_config()\n        base_path = config[\"data_storage_path\"]\n\n    if not records:\n        logger.info(\"No records to save\")\n        return\n\n    df = pd.DataFrame(records)\n\n    # Extract consumption date from timestamp for partitioning\n    df[\"consumption_date\"] = df[\"timestamp\"].dt.date\n\n    # Group by metering point and consumption date\n    for (mp_id, consumption_date), group_df in df.groupby(\n        [\"metering_point_id\", \"consumption_date\"]\n    ):\n        try:\n            # Create partitioned directory structure\n            partition_path = (\n                Path(base_path) / f\"metering_point={mp_id}\" / f\"date={consumption_date}\"\n            )\n            partition_path.mkdir(parents=True, exist_ok=True)\n\n            file_path = partition_path / \"consumption_data.parquet\"\n\n            # Upsert logic: check if file exists\n            if file_path.exists():\n                # Load existing data\n                existing_df = pd.read_parquet(file_path)\n                # Remove rows with timestamps that overlap with new data\n                existing_df = existing_df[~existing_df[\"timestamp\"].isin(group_df[\"timestamp\"])]\n                # Combine old (without duplicates) + new data and sort\n                combined_df = pd.concat([existing_df, group_df], ignore_index=True)\n                combined_df = combined_df.sort_values(\"timestamp\")\n            else:\n                combined_df = group_df.sort_values(\"timestamp\")\n\n            # Drop the temporary consumption_date column before saving\n            combined_df = combined_df.drop(columns=[\"consumption_date\"])\n\n            combined_df.to_parquet(file_path, engine=\"pyarrow\", compression=\"snappy\", index=False)\n\n            logger.info(\"Wrote %d records to %s\", len(combined_df), file_path)\n        except Exception as e:\n            logger.error(\"Failed to write Parquet file for %s/%s: %s\", mp_id, consumption_date, e)\n            raise\n</code></pre>"},{"location":"reference/#powerview.src.storage.update_last_ingestion_date","title":"<code>update_last_ingestion_date(metering_point_id, date_to, db_path='./state.duckdb')</code>","text":"<p>Update the last ingestion date for a metering point in DuckDB.</p> <p>Uses upsert (insert or replace) logic. Retries up to 3 times on failure.</p> <p>Parameters:</p> Name Type Description Default <code>metering_point_id</code> <code>str</code> <p>The metering point ID to update.</p> required <code>date_to</code> <code>date</code> <p>The new last ingestion date.</p> required <code>db_path</code> <code>str</code> <p>Path to the DuckDB database file.</p> <code>'./state.duckdb'</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If update fails after 3 retries.</p> Source code in <code>powerview/src/storage.py</code> <pre><code>def update_last_ingestion_date(\n    metering_point_id: str, date_to: date, db_path: str = \"./state.duckdb\"\n) -&gt; None:\n    \"\"\"\n    Update the last ingestion date for a metering point in DuckDB.\n\n    Uses upsert (insert or replace) logic. Retries up to 3 times on failure.\n\n    Args:\n        metering_point_id: The metering point ID to update.\n        date_to: The new last ingestion date.\n        db_path: Path to the DuckDB database file.\n\n    Raises:\n        Exception: If update fails after 3 retries.\n    \"\"\"\n    retries = 0\n    max_retries = 3\n    while retries &lt; max_retries:\n        try:\n            con = duckdb.connect(db_path)\n            con.execute(\n                \"\"\"\n                INSERT INTO ingestion_state (metering_point_id, last_ingestion_date)\n                VALUES (?, ?)\n                ON CONFLICT(metering_point_id) DO\n                UPDATE SET last_ingestion_date=excluded.last_ingestion_date\n                \"\"\",\n                [metering_point_id, date_to],\n            )\n            con.close()\n            logger.info(\"Updated last ingestion date for %s to %s\", metering_point_id, date_to)\n            return\n        except Exception as e:\n            retries += 1\n            if retries &gt;= max_retries:\n                logger.error(\n                    \"Failed to update last ingestion date after %d retries: %s\",\n                    max_retries,\n                    e,\n                )\n                raise\n            logger.warning(\"DuckDB update failed, retry %d/%d\", retries, max_retries)\n</code></pre>"},{"location":"reference/#metadata","title":"Metadata","text":"<p>Utilities for loading metering-point metadata into DuckDB.</p>"},{"location":"reference/#powerview.src.metadata.build_metadata_frame","title":"<code>build_metadata_frame(metering_points)</code>","text":"<p>Convert metering-point metadata into a normalized pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>metering_points</code> <code>dict[str, dict[str, Any]]</code> <p>Mapping of metering point IDs to metadata dictionaries.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pandas.DataFrame: Normalized metadata ready for DuckDB ingestion.</p> Source code in <code>powerview/src/metadata.py</code> <pre><code>def build_metadata_frame(metering_points: dict[str, dict[str, Any]]) -&gt; pd.DataFrame:\n    \"\"\"Convert metering-point metadata into a normalized pandas DataFrame.\n\n    Args:\n        metering_points: Mapping of metering point IDs to metadata dictionaries.\n\n    Returns:\n        pandas.DataFrame: Normalized metadata ready for DuckDB ingestion.\n    \"\"\"\n\n    records: list[dict[str, Any]] = []\n    for meter_id, metadata in sorted(metering_points.items()):\n        normalized = metadata or {}\n        extras = {\n            key: value for key, value in normalized.items() if key not in _CORE_METADATA_FIELDS\n        }\n        records.append(\n            {\n                \"metering_point_id\": meter_id,\n                \"name\": normalized.get(\"name\", meter_id),\n                \"type\": normalized.get(\"type\"),\n                \"location\": normalized.get(\"location\"),\n                \"description\": normalized.get(\"description\"),\n                \"extra_metadata\": json.dumps(extras, ensure_ascii=True) if extras else None,\n            }\n        )\n\n    if not records:\n        return pd.DataFrame(\n            columns=[\n                \"metering_point_id\",\n                \"name\",\n                \"type\",\n                \"location\",\n                \"description\",\n                \"extra_metadata\",\n            ]\n        )\n\n    return pd.DataFrame.from_records(records)\n</code></pre>"},{"location":"reference/#powerview.src.metadata.load_metadata_table","title":"<code>load_metadata_table(connection, metering_points, table_name='reporting.meter_metadata')</code>","text":"<p>Create or replace the DuckDB metadata table from YAML-derived data.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>DuckDBPyConnection</code> <p>Active DuckDB connection for the analytics database.</p> required <code>metering_points</code> <code>dict[str, dict[str, Any]]</code> <p>Mapping of metering point IDs to metadata dictionaries.</p> required <code>table_name</code> <code>str</code> <p>Fully qualified table name to create/replace.</p> <code>'reporting.meter_metadata'</code> Source code in <code>powerview/src/metadata.py</code> <pre><code>def load_metadata_table(\n    connection: duckdb.DuckDBPyConnection,\n    metering_points: dict[str, dict[str, Any]],\n    table_name: str = \"reporting.meter_metadata\",\n) -&gt; None:\n    \"\"\"Create or replace the DuckDB metadata table from YAML-derived data.\n\n    Args:\n        connection: Active DuckDB connection for the analytics database.\n        metering_points: Mapping of metering point IDs to metadata dictionaries.\n        table_name: Fully qualified table name to create/replace.\n    \"\"\"\n\n    metadata_frame = build_metadata_frame(metering_points)\n    connection.execute(\"CREATE SCHEMA IF NOT EXISTS reporting\")\n\n    connection.register(\"meter_metadata_df\", metadata_frame)\n    try:\n        connection.execute(\n            f\"\"\"\n            CREATE OR REPLACE TABLE {table_name} AS\n            SELECT * FROM meter_metadata_df\n            \"\"\"\n        )\n    finally:\n        connection.unregister(\"meter_metadata_df\")\n\n    logger.info(\n        \"Loaded %d metering-point metadata row(s) into %s\",\n        len(metadata_frame.index),\n        table_name,\n    )\n</code></pre>"},{"location":"reference/#reporting","title":"Reporting","text":"<p>Reporting layer builder for analytics DuckDB.</p>"},{"location":"reference/#powerview.src.reporting.build_reporting_layer","title":"<code>build_reporting_layer(*, data_path=None, analytics_db_path=None, metering_points_override=None)</code>","text":"<p>Build or refresh the DuckDB reporting layer defined in the plan.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str | Path | None</code> <p>Optional override for the Parquet data root.</p> <code>None</code> <code>analytics_db_path</code> <code>str | Path | None</code> <p>Optional override for the DuckDB database location.</p> <code>None</code> <code>metering_points_override</code> <code>dict[str, dict[str, Any]] | None</code> <p>Optional metadata mapping to bypass config loading.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Location of the analytics DuckDB file.</p> Source code in <code>powerview/src/reporting.py</code> <pre><code>def build_reporting_layer(\n    *,\n    data_path: str | Path | None = None,\n    analytics_db_path: str | Path | None = None,\n    metering_points_override: dict[str, dict[str, Any]] | None = None,\n) -&gt; Path:\n    \"\"\"Build or refresh the DuckDB reporting layer defined in the plan.\n\n    Args:\n        data_path: Optional override for the Parquet data root.\n        analytics_db_path: Optional override for the DuckDB database location.\n        metering_points_override: Optional metadata mapping to bypass config loading.\n\n    Returns:\n        Path: Location of the analytics DuckDB file.\n    \"\"\"\n\n    config = load_config()\n    resolved_data_path = Path(data_path or config[\"data_storage_path\"]).resolve()\n    resolved_analytics_path = Path(analytics_db_path or config[\"analytics_db_path\"]).resolve()\n    metadata = metering_points_override or config[\"metering_points\"]\n\n    if not resolved_data_path.exists():\n        raise FileNotFoundError(f\"Data path does not exist: {resolved_data_path}\")\n\n    resolved_analytics_path.parent.mkdir(parents=True, exist_ok=True)\n    parquet_glob, files = _parquet_glob(resolved_data_path)\n    logger.info(\"Found %d parquet partition(s) for reporting\", len(files))\n\n    connection = duckdb.connect(resolved_analytics_path.as_posix())\n    try:\n        connection.execute(\"CREATE SCHEMA IF NOT EXISTS reporting\")\n        for view_name, statement in _view_statements(parquet_glob):\n            connection.execute(statement)\n            logger.info(\"Created or replaced view %s\", view_name)\n\n        load_metadata_table(connection, metadata)\n        connection.execute(_METADATA_ENRICHED_VIEW)\n        logger.info(\"Created metadata-enriched daily view\")\n    finally:\n        connection.close()\n\n    logger.info(\"Reporting layer refreshed at %s\", resolved_analytics_path)\n    return resolved_analytics_path\n</code></pre>"},{"location":"reference/#pipeline","title":"Pipeline","text":"<p>Main orchestration module for Eloverblik data extraction pipeline.</p>"},{"location":"reference/#powerview.src.main.main","title":"<code>main()</code>","text":"<p>Main extraction workflow orchestrating all components.</p> <p>Coordinates configuration loading, API authentication, data extraction, normalization, and storage. Handles errors gracefully to ensure individual metering point or chunk failures do not stop entire run.</p> Source code in <code>powerview/src/main.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"\n    Main extraction workflow orchestrating all components.\n\n    Coordinates configuration loading, API authentication, data extraction,\n    normalization, and storage. Handles errors gracefully to ensure\n    individual metering point or chunk failures do not stop entire run.\n    \"\"\"\n    # Load configuration\n    try:\n        config = load_config()\n    except ValueError as e:\n        logger = logging.getLogger(__name__)\n        logger.error(\"Configuration error: %s\", e)\n        return\n\n    # Configure logging\n    logging.basicConfig(\n        level=getattr(logging, config[\"log_level\"]),\n        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    )\n    logger = logging.getLogger(__name__)\n\n    logger.info(\"Starting Eloverblik data extraction\")\n\n    # Initialize state database\n    try:\n        init_duckdb_state(config[\"state_db_path\"])\n    except Exception as e:\n        logger.error(\"Failed to initialize DuckDB state: %s\", e)\n        return\n\n    # Get access token (fresh for each run)\n    try:\n        access_token = get_access_token(config[\"refresh_token\"])\n    except Exception as e:\n        logger.error(\"Failed to obtain access token: %s\", e)\n        return\n\n    logger.info(\"Tracking %d metering point(s)\", len(config[\"valid_metering_points\"]))\n\n    # Extract data for each metering point\n    for mp_name, mp_id in config[\"valid_metering_points\"].items():\n        logger.info(\"Processing %s (%s)\", mp_name, mp_id)\n\n        try:\n            # Get timeframe\n            date_from, date_to = get_timeframe(\n                mp_id, config[\"initial_backfill_days\"], config[\"state_db_path\"]\n            )\n\n            # Chunk large date ranges\n            chunks = chunk_date_range(date_from, date_to, chunk_days=90)\n\n            # Track if any chunk succeeded for this metering point\n            chunk_success = False\n\n            # Extract data for each chunk\n            for chunk_from, chunk_to in chunks:\n                logger.info(\"Fetching data from %s to %s\", chunk_from, chunk_to)\n\n                try:\n                    # Fetch from API\n                    api_response = get_meter_data_with_retry(\n                        access_token,\n                        chunk_from.isoformat(),\n                        chunk_to.isoformat(),\n                        metering_point_ids=[mp_id],\n                    )\n\n                    # Normalize\n\n                    records = normalize_api_response(api_response, config[\"valid_metering_points\"])\n\n                    # Save to Parquet\n                    save_to_parquet(records, config[\"data_storage_path\"])\n\n                    # Mark this metering point as having successful chunk\n                    chunk_success = True\n\n                except Exception as e:\n                    logger.error(\n                        \"Failed to fetch/process data for chunk %s to %s: %s\",\n                        chunk_from,\n                        chunk_to,\n                        e,\n                    )\n                    continue\n\n            # Only update state if at least one chunk succeeded for this metering point\n            if chunk_success:\n                update_last_ingestion_date(mp_id, date_to, config[\"state_db_path\"])\n                logger.info(\"Completed processing for %s\", mp_name)\n            else:\n                logger.warning(\n                    \"No chunks succeeded for %s (%s). State not updated. \"\n                    \"Will retry from same date range on next run.\",\n                    mp_name,\n                    mp_id,\n                )\n\n        except Exception as e:\n            logger.error(\"Failed to process %s: %s\", mp_name, e)\n            continue\n\n    logger.info(\"Extraction workflow completed\")\n</code></pre>"},{"location":"superset/","title":"Superset","text":"<p>Superset runs via Docker Compose in the superset/ directory.</p>"},{"location":"superset/#setup","title":"Setup","text":"<p>1) Copy the example env file and set a password.</p> <pre><code>cp superset/.env.example superset/.env\n</code></pre> <p>2) Start containers.</p> <pre><code>cd superset\ndocker compose up -d\n</code></pre> <p>3) Initialize the Superset database and admin user.</p> <pre><code>docker compose exec superset superset db upgrade\ndocker compose exec superset superset fab create-admin \\\n    --username admin \\\n    --firstname Admin \\\n    --lastname User \\\n    --email admin@superset.com \\\n    --password admin\ndocker compose exec superset superset init\n</code></pre>"},{"location":"superset/#access","title":"Access","text":"<p>Open http://localhost:8088 and sign in with the credentials you created.</p>"},{"location":"superset/#connect-to-duckdb","title":"Connect to DuckDB","text":"<p>1) In Superset, go to Settings \u2192 Database Connections \u2192 + Database. 2) Select DuckDB. 3) Use the SQLAlchemy URI below.</p> <pre><code>duckdb:////app/duckdb/analytics.duckdb\n</code></pre> <p>If you want to query Parquet directly, use:</p> <pre><code>duckdb:////app/external_data\n</code></pre>"},{"location":"superset/#environment","title":"Environment","text":"<p>Check superset/.env for settings used by Docker Compose.</p>"}]}